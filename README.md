## Milestone 3
The code starts by preparing the data for analysis. It uses a specific DataFrame, hourly_avg_temp, which includes the average hourly temperature. The 'hour' column is separated from the features for later use. The features are then scaled to a range between 0 and 1 using the MinMaxScaler from sklearn, resulting in a normalized dataset which can help compare the two different information with each other and how it relates to time. The processed features and the 'hour' column are then concatenated to form the final DataFrame, final_df.

Next, the code sets up the neural network for training. It imports necessary libraries such as pandas, numpy, and various modules from keras for building the neural network. The prepared data (processed_df) is used as the input features (X), while the original features are used as the target labels (y). The dataset is split into training and test sets, with 10% of the data reserved for testing. A sequential neural network model is built with one input layer, two hidden layers, each containing 12 neurons and using the sigmoid activation function. The output layer also uses the sigmoid activation function. The model is compiled using the SGD optimizer with a learning rate of 0.3 and the categorical crossentropy loss function, and then trained for 100 epochs with a batch size of 32 and a validation split of 10%.

After training, the model's performance is evaluated on the test set. Predictions are made using the trained model, and the true and predicted class labels are determined. A confusion matrix is computed to assess the performance of the model. This matrix is visualized using ConfusionMatrixDisplay from sklearn, displaying the results in a heatmap format. The confusion matrix helps in understanding the accuracy and error rates of the model's predictions.
In summary, the code involves a comprehensive process of data preparation, neural network training, and performance evaluation. The data is first merged and normalized, and then a neural network is built and trained to predict weather-related features. The model's accuracy is assessed using a confusion matrix, providing insights into the model's predictive capabilities and highlighting areas for potential improvement. This process demonstrates the application of machine learning techniques to analyze and predict complex weather patterns.

With our results, we noticed that our model has around an accuracy of 5.8912^-6 which means our model is underfitting. This means that we have to improve our model to have a higher accuracy to have our model to be appropriate-fitting. Another improvement we can make is to do hyperparameter tuning to determine which parameters are the best suited for our model.

## Milestone 2
We preprocessed our data to answer the question: since 2000, has the Earth gotten hotter? We'll do this by comparing every hourly collectively since 2000, to see if they steadily increase. We started by collecting a dataset called 'weather' with 43 features and over 261,000 rows. Given the abundance of data, we had to process it to our liking. First, we made a list of features we wanted to drop since they do not correlate with answering our question. These features were: ['Unnamed: 32', 'Unnamed: 41', 'metar', 'metar_origin', 'pressure_change_code', 'weather_cond_code', 'pressure_change_code', 'visibility', 'cloud_layer_1', 'cloud_layer_2', 'cloud_layer_3', 'wind_cardinal_direction', 'precip_accum_one_hour', 'cloud_layer_1_code', 'cloud_layer_2_code', 'cloud_layer_3_code', 'heat_index']. After removing the unnecessary features, we addressed the issue of missing data by dropping NA values from our main column, 'air_temp', and then filling in every numeric column with NA values using the corresponding column mean. We converted 'air_temp' to a numeric column to ensure it was in the correct format.

Next, we converted and changed the format of the 'date_time' column in 'weather.csv' to 'pd.to_datetime', allowing us to utilize the date more precisely. Using the 'date_time' column, we created four additional columns: 'year', 'month', 'day', and 'hour'. With these new columns, we calculated each hourly average temperature by grouping by 'year' and 'month', ‘day’, and ‘hour’ and calculating the mean.

In our first graph. The code uses Python's `matplotlib` and `seaborn` to create a line graph showing how three-hour accumulated precipitation changes over the years. First, it sets up the plot and sorts the data by year. It then plots the years on the x-axis and the precipitation values on the y-axis using a green line. The graph shows a sharp drop in precipitation around 2000, followed by ups and downs, and a slight increase in recent years. This helps visualize the trend of three-hour precipitation over time.

In our second graph. We also used Python's `matplotlib` and `seaborn` to create a line graph showing how air temperature changes over the years. First, it sets up the plot and sorts the data by year. It then plots the years on the x-axis and the air temperature values on the y-axis using a red line. The graph shows fluctuations in air temperature from 1998 to 2025, with notable peaks and troughs. This helps visualize the trend of air temperature over time.

For our third graph. The code uses `matplotlib` to create a line graph showing monthly average temperatures over time. It sets up a large figure, then plots the year and month on the x-axis and the average temperature on the y-axis using blue circles and lines. The graph shows regular seasonal cycles with temperatures rising and falling each year. There are clear peaks and troughs, indicating the typical seasonal changes. The labels and title help explain the graph, making it easy to see how average temperatures have varied over the years.

For our fourth graph we did a scatter plot. This code uses `matplotlib` to create a scatter plot showing yearly average temperatures over time. It sets up a figure, then plots the years on the x-axis and the average temperatures on the y-axis using blue dots. The plot is labeled to indicate that the data points represent actual temperature data. The x-axis is labeled 'Year' and the y-axis is labeled 'Yearly Average Temperature.' The graph shows individual data points for each year's average temperature, revealing trends and variations over time. The grid and legend make it easier to read and understand the plot. This visual representation helps in identifying patterns in yearly average temperatures from 2000 to 2025.

For our fifth graph we did a pairplot. The code merges weather data with monthly average temperatures, converts the 'date_time' column to extract year, month, day, and hour, and converts specified columns to numeric types. It then drops rows with missing values in these columns. Finally, it creates a pair plot using `seaborn` to show scatter plots and histograms of the relationships between different weather variables. The pair plot helps visualize how these variables, such as temperature, humidity, and wind speed, relate to each other.

For our last graph we did a correlation matrix. The code calculates the correlation matrix for various weather variables and visualizes it using a heatmap. The correlation matrix shows how strongly each pair of variables, such as temperature, humidity, and wind speed, are related. The heatmap uses colors to represent the correlation values, with annotations displaying the exact values. This helps quickly identify which variables have strong positive or negative correlations.
